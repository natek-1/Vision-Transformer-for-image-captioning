{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "from tqdm  import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    '''\n",
    "    Turns a 2D input image into a 1D sequence learnable embedding vector.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of color channels for the input images. Defaults to 3.\n",
    "        patch_size (int): Size of patches to convert input image into. Defaults to 16.\n",
    "        embedding_dim (int): Size of embedding to turn image into. Defaults to 768.\n",
    "    '''\n",
    "    def __init__(self, in_channels = 3, patch_size=16, embedding_dim=768):\n",
    "        super().__init__()\n",
    "        self.patcher = nn.Conv2d(in_channels=in_channels, out_channels=embedding_dim,\n",
    "                                 kernel_size=patch_size, stride=patch_size, padding=0)\n",
    "        \n",
    "        self.flatten = nn.Flatten(start_dim=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # input of shape (batch_size, color_channel, height, width)\n",
    "        x = self.patcher(x) # (batch_size, embedding_dim, height//patch_size, width//patch_size)\n",
    "        x = self.flatten(x) # (batch_size, embedding_dim, (height * width)//(patch_size)**2)\n",
    "        return x.permute(0, 2, 1) #(batch_size,(height * width)//(patch_size)**2), embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlcok(nn.Module):\n",
    "    '''\n",
    "    Encoder block that returns a representation of the image patches.\n",
    "    \n",
    "    Args:\n",
    "        embedding_dim (int): size of the embedding for each image patch. Defaults 768.\n",
    "        num_heads (int): Number of head in the attention layer. Defaults 12.\n",
    "        mlp_size (int): Size for the feed forward portion of the encoder. Defaults 3072.\n",
    "        dropout (float): Amount of dropout in attention and mlp layer. Default 0.1\n",
    "    '''\n",
    "    def __init__(self, embeding_dim = 768, num_heads = 12, mlp_size= 3072, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_norm1 = nn.LayerNorm(normalized_shape=embeding_dim)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embeding_dim, num_heads=num_heads, dropout=dropout)\n",
    "        \n",
    "        self.layer_norm2 = nn.LayerNorm(normalized_shape=embeding_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embeding_dim, mlp_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_size, embeding_dim),\n",
    "            nn.Dropout(dropout)   \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        norm_x = self.layer_norm1(x)\n",
    "        x = x + self.attention(norm_x, norm_x, norm_x, need_weights=False)[0]\n",
    "        \n",
    "        norm_x = self.layer_norm2(x)\n",
    "        return x + self.mlp(norm_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    '''\n",
    "    Creates a Vision Transformer architecture with ViT-Base hyperparameters by default with no classification token and layer.\n",
    "    '''\n",
    "    def __init__(self, img_size= 224, in_channels=3, patch_size=16, num_blocks = 12,\n",
    "                 embed_dim = 768, mlp_size = 3072, num_heads = 12, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.position_embedding = nn.Parameter(torch.randn(1, self.num_patches, embed_dim))\n",
    "        self.embed_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.patch_embedding = PatchEmbedding(in_channels=in_channels, patch_size=patch_size, embedding_dim=embed_dim)\n",
    "        \n",
    "        self.transformer_encoder = nn.Sequential(*[EncoderBlcok(embeding_dim=embed_dim, num_heads=num_heads,\n",
    "                                                                mlp_size=mlp_size, dropout=dropout) for _ in range(num_blocks)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is an image of shape (batch_size, color_chanel, height, width)\n",
    "        x = self.patch_embedding(x) # -> (batch_size, embed_dim, num_patches)\n",
    "        x = x + self.position_embedding\n",
    "        x = self.embed_dropout(x)\n",
    "        return self.transformer_encoder(x) # -> (batch_size, embed_dim, num_patches)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder model (GPT-2 like model requires cross attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim=768, num_heads=12, dropout=0.1, mlp_size=3072):\n",
    "        super().__init__()\n",
    "        self.mask_attention = nn.MultiheadAttention(embed_dim, num_heads, dropout, batch_first=True)\n",
    "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim, num_heads, dropout, batch_first=True)\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_size, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, x, encoder_out, padding_mask):\n",
    "        seq_len = x.shape[1]\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1).bool()\n",
    "        att_x, _ = self.mask_attention(x, x, x, attn_mask=mask, need_weights=False, key_padding_mask=padding_mask)\n",
    "        x = self.layer_norm1(x + att_x)\n",
    "        \n",
    "        att_x, _ = self.cross_attention(x, encoder_out, encoder_out, need_weights=False)\n",
    "        x = self.layer_norm2(x + att_x)\n",
    "        \n",
    "        return self.mlp(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim = 768, mlp_size = 3072, max_seq_len=350, num_layers=12, num_heads=12, dropout=0.1, padding_idx=0):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim=embed_dim)\n",
    "        self.positional_embedding = nn.Embedding(max_seq_len, embedding_dim=embed_dim)\n",
    "        \n",
    "        self.transformer_decoder = nn.ModuleList(*[DecoderBlock(embed_dim, num_heads, dropout, mlp_size) for _ in range(num_layers)])\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "        self.padding_index = padding_idx\n",
    "    \n",
    "    def forward(self, x, encoder_out):\n",
    "        # x of shape (batch_size, seq_len)\n",
    "        _, seq_len = x.shape\n",
    "        padding_key = (x == self.padding_index)\n",
    "        pos = torch.arange(0, seq_len, dtype=torch.long, device=x.device)\n",
    "        pos_embed = self.positional_embedding(pos) # shape (seq_len, n_emebd)\n",
    "        token_embed = self.token_embedding(x) # output of shape (batch_size, seq_len, embed_dim)\n",
    "        x = pos_embed + token_embed\n",
    "        \n",
    "        for decoder in self.transformer_decoder:\n",
    "            x = decoder(x, encoder_out, padding_key)\n",
    "            \n",
    "        return self.fc_out(self.layer_norm(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_captioning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
